---
title: 高性能集群搭建指南（二） - 系统配置
date: 2017-05-15 22:05:49
tags:
categories:
- Linux
---

# 系统配置

这里用四台虚拟机测试，以下是它的IP和主机名  
（其实我们并没有mic卡 QAQ，最开始接触服务器的时候主机名叫 `mic1`，之后都一直这么命名成习惯了）  

服务端：mic1   
客户端：mic2 - mic4

电脑上的虚拟机坏了 (x  x) 此处暂时略过

<br>

---
## 一、hostname hosts 和 IP

### 1. hostname 设定

`hostname` 为本机主机名，操作系统会根据  `/etc/hostname` 中的内容来设置主机名  
由于hostname涉及到集群之间互联，因此一台机器的 `hostname` 和其他机器中 `host` 文件中对应的 `hostname` 应保持一致。

#### 1.1 暂时更改 `hostname` 

``` bash
$ hostname <your_hostname>
$ su -l #重新登录即生效
``` 

实际测试如下
``` bash
[root@mic1 ~]# hostname         #查看主机名
mic1
[root@mic1 ~]# hostname 2333    #修改
[root@mic1 ~]# su -l            #重新登录
Last login: Fri May 19 01:02:10 CST 2017 on pts/0
Intel(R) Parallel Studio XE 2017 Update 1 for Linux*
Copyright (C) 2009-2016 Intel Corporation. All rights reserved.
[root@2333 ~]# hostname
2333
[root@2333 ~]#
```


#### 1.2 永久更改 `hostname`

将以下内容下入 `/etc/hostname` 中，在系统下次重启后生效。

``` bash
<your_hostname>
```
<br>

### 2. hosts

目前的以太网使用的是的 `TCP/IP` 协定，其中IP为第四版的 `IPv4` 。`IPv4` 为32 位，为了人脑易读已经转成四组十进制的数字了，每组为 `0-255`，例如 `12.34.56.78` 这样的格式。当我们利用 Internet 传送数据的时候，就需要这个IP来找到对应的主机。

然而IP这种数字不易记忆，为了应付这个问题，早期的开发者想到一个方法，那就是利用某些特定的文件将主机名称与IP作一个对应，如此一来，我们就可以同过主机名称来取得该主机的IP了！在linux中，这个文件为 `/etc/hosts`。（Winodws中为C:\Windows\System32\drivers\etc\hosts）

我们需要修改 `/etc/hosts` 中的内容，让集群之间通过主机名来相互识别。可以在服务端写好该文件之后把它分发到其余节点上。

以下是一个样例：


``` fs
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

#addbylq 20170327
10.111.0.101 mic1
10.111.0.102 mic2
10.111.0.103 mic3
10.111.0.104 mic4
10.111.0.105 mic5
10.111.0.106 mic6
10.111.0.107 mic7
10.111.0.108 mic8
10.111.0.109 mic9
10.111.0.110 mic10
10.111.0.111 mic11
10.111.0.112 mic12
```
<br>

### 3. IP

在成功安装完成CentOS后，配置好网络配置就可以远程连接这些机器进行后面的配置了，这里先介绍以太网的配置。

!!!!2 3  明天继续补充, 有IB的话应该直接先配IB的网络

<br>
<br>
<br>

## 二、SELinux和防火墙

把SELinux放到第一个来写，主要是因为被SELinux坑过太多次了，并且它也会影响后面的配置工作，详细说明见 `man selinux` 。  

建议在装机完成后马上关闭SELinux。
<br>
<br>


### 1. SELinux模式

>`SELinux` 更能遵从最小权限的理念。在缺省的 `Enforcing` 情况下，一切均被拒绝，接着有一系列例外的政策来允许系统的每个元素（服务、程序、用户）运作时所需的访问权。当一项服务、程序或用户尝试访问或修改一个它不须用的文件或资源时，它的请求会遭拒绝，而这个行动会被记录下来。
  
由于SELinux未关闭，在搭本地源的时候出现过客户端无法访问的情况。


>`SELinux` 拥有三个基本的操作模式，当中 `enforcing` 是缺省的模式。此外，它还有一个 targeted 或 mls 的修饰语。这管制 `SELinux` 规则的应用有多广泛，当中 targeted 是较宽松的级别。  
>
>- **enforcing**： 这个缺省模式会在系统上启用并实施 `SELinux` 的安全性政策，拒绝访问及记录行动。
>- **permissive**： 在 `permissive` 模式下，`SELinux` 会被启用但不会实施安全性政策，而只会发出警告及记录行动。`permissive` 模式在排除 SELinux 的问题时很有用。
>- **disabled**： `SELinux` 已被停用。
<br>
<br>

### 2. 关闭SELinux

`SELinux` 的配置文件为 `/etc/selinux/config` 
``` bash
$ vim /etc/selinux/config

#将SELinux模式改为disabled
SELINUX=disabled  #默认为enforcing
```
<br>
<br>

### 3. 查看SELinux工作 状态

`sestatus` 或 `getenforce`
``` bash
$ sestatus

$ getenforce
```
<br>
<br>

### 4. 关于 setenforce
  
`setenforce 0` 将SELinux临时改为 `permissive` 模式，注意并非是 `disabled` 模式。

>DESCRIPTION
>- Use Enforcing or 1 to put SELinux in **enforcing** mode.
>- Use Permissive or 0 to put SELinux in **permissive** mode.  

<br>
<br>

### 5. 几个链接

<br>

### 6. 关闭防火墙

防火墙关闭后的安全问题有待探讨

``` bash
$ systemctl stop    firewalld
$ systemctl disable firewalld
```   
<br>
<br>
<br>

## 三、yum 本地源 （可选）

这里先介绍一下CentOS的软件包管理工具 `yum` 

>Yum（Yellow dog Updater, Modified）由Duke University团队，修改Yellow Dog Linux的Yellow Dog Updater开发而成，是一个基于RPM包管理的字符前端软件包管理器。能够从指定的服务器自动下载RPM包并且安装，可以处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。被Yellow Dog Linux本身，以及Fedora、Red Hat Enterprise Linux采用。

>FILES
>- /etc/yum.conf
>- /etc/yum/version-groups.conf
>- /etc/yum.repos.d/
>- /etc/yum/pluginconf.d/
>- /var/cache/yum/   

<br>
<br>

### 1. 服务端配置

#### 1.1 准备rpm包

- 将 Centos-Base 源同步至本地目录

在 `CentOS-7-x86_64-Everything-1611.iso` 中有 yum 源中的全部rpm包，故将其中的软件包全部拷到本地即可。

``` shell
$ mount /dev/cdrom /mnt
$ mkdir /yum
$ cp -r /mnt/Packages /yum/base
```

这里也提供另一种用 `reposync` 指令来同步rpm包到本地目录的方法,执行以下脚本即可


``` bash
#!/bin/bash
cat << EOF > sync-centos.sh
#!/bin/bash
#
# reposync
#

BASEDIR=/yum/
mkdir -p $BASEDIR 
cd $BASEDIR

reposync -n -r updates
repomanage -o -c updates | xargs rm -fv
createrepo updates

reposync -n -r base --downloadcomps
repomanage -o -c base | xargs rm -fv
createrepo base -g comps.xml
EOF

chmod 755 sync-centos.sh
sh sync-centos.sh
```

- 将epel源同步至本地目录

执行以下脚本即可

``` bash
#!/bin/bash
#epel源更新脚本

#安装epel包
rpm  -ivh http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-9.noarch.rpm
#yum install epel-release -y
cd /yum/

cat << EOF > sync-epel.sh
#!/bin/bash

BASEDIR=/yum/
mkdir -p \$BASEDIR 
cd \$BASEDIR

reposync -n -r epel
repomanage -o -c epel | xargs rm -fv
createrepo epel
EOF

chmod 755 sync-epel.sh
sh sync-epel.sh
```

在无网络情况，可以把提前准备好的 `epel本地源` 复制到 `/yum/epel` 目录下  

<br>

#### 1.2 配置repo

- 先做好备份

``` bash
$ cp -r /etc/yum.repos.d /etc/yum.repos.d.bak
$ cd /etc/yum.repos.d
$ rm -f ./*
```

- CentOS-Base源

``` bash
$ vim CentOS-Server.repo
```

将以下内容写入 `CentOS-Server.repo`

```
[CentOS-Base]
name=CentOS-$releasever - Base
baseurl=file:///yum/base/
enable=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
```

`Base` 写为其他也可以，`gpgcheck` 为软件包签名的验证，以防软件包损坏或被篡改，这里选择关闭。

- epel源


``` bash
$ vim /etc/yum.repos.d/epel-Server
```

将以下内容写入 `epel-Server`

```
[epel-Server]
name=epel-Server
baseurl=file:///yum/epel
enable=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
```  
<br>

#### 1.3 更新软件包缓存 

``` bash
$ yum clean all
$ yum makecache
```

这样服务端的 yum 源就配置好了，可以简单地验证是否正常。

``` bash
$ yum install tree -y
```
<br>

#### 1.4 ftp服务器配置

我们通过在服务端的 ftp服务，实现局域网的 yum源的搭建。

- ftp 安装

``` bash
$ yum install vsftpd -y
```

- ftp 配置  
编辑 `/etc/vsftpd/vsftpd.conf`

``` bash
$ echo "anon_root=/yum/" >> /etc/vsftpd/vsftpd.conf
```

- ftp 服务启动

``` bash
$ systemctl start  vsftpd       #启动服务
$ systemctl enable vsftpd       #启用开机启动
```



### 2. 客户端

客户端的配置相对来说会快很多，只需要修改`repo` 文件即可  


#### 2.1 配置 repo

``` bash
$ cp -r /etc/yum.repos.d /etc/yum.repos.d.bak
$ cd /etc/yum.repos.d
$ rm -f ./*
```

- CentOS-Base源

``` bash
$ vim CentOS-Client.repo
``` 

将以下内容写入 `CentOS-Client.repo`

``` 
[CentOS-Media]
name=CentOS-$releasever - Media
baseurl=ftp://<hostname>/base      #<hostname>也可以写IP
enable=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
```

- epel源

``` bash
$ vim /etc/yum.repos.d/epel-Client
```

将以下内容写入 `epel-Client`

```
[epel-Client]
name=epel-Client
baseurl=ftp://<hostname>/epel      #<hostname>也可以写IP
enable=1
gpgcheck=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7
```
<br>

#### 2.2 更新软件包缓存 

``` bash
$ yum clean all
$ yum makecache
```
<br>
<br>
<br>


## 三、SSH无密码访问

SSH是一种网络协议，用于计算机之间的加密登录。因为受版权和加密算法的限制，现在大多使用OpenSSH。OpenSSH是SSH的替代软件，而且是开源的。

http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html

>ssh的工作机制
>1. 远程主机收到用户的登录请求，把自己的公钥发给用户。
>2. 用户使用这个公钥，将登录密码加密后，发送给远程主机。
>3. 远程主机用自己的私钥，解密登录密码，如果密码正确，则允许用户登录。  

<br>

>ssh中的文件  
>1. id_rsa   
ssh私钥，用于解密登录密码
>2. id_rsa.pub  
ssh公钥，用于加密
>3. authorized_keys  
保存了已经认证的主机的公钥指纹，`authorized_keys` 保证了被登录时的安全性。
>4. known_hosts
远程主机的公钥指纹，RSA公钥通过MD5计算产生当前公钥的指纹，用于提供给用户验证远程主机的密钥是否未被修改。`known_hosts` 保证了登录其他机器时的安全性。

<br>

### 1. OpenSSH 安装

``` bash
$ yum install openssh -y
```

<br>

### 2. 单机的无密码访问配置


实现无密码访问的条件是本机公钥存在于远程主机 `authorized_keys` 中。

我们利用 `ssh-keygen` 生成密钥 `id_rsa` 和 `id_rsa.pub` ， 再通过 `ssh-copy-id` 将公钥拷贝至远程主机的 `authorized_keys` ，即可实现对远程主机的无密码访问。

需要注意的是，无论是本地还是远程，无密码访问都是对用户而言的，而非是整个机器的所有用户。

``` bash
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ ssh-copy-id [user@]hostname    
```

当然，拷贝公钥也可以通过 `scp` 来实现。


如果该用户先前已经生成了一组密钥，若重新生成，则公钥指纹也会被修改。因此想通过ssh连接该主机的用户需要把 `known_hosts` 中对应的远程机器的公钥指纹删除。

<br>

### 3. 集群的无密码访问配置

整体思路先在所有机器中利用 `ssh-keygen` 产生密钥。然后在一台机器上，实现其对本机和其他机器各用户的无密码访问，接着把本机和其他机器的公钥复制至本机的 `authorized_keys` 中。最后把该 `autorized_keys` 文件复制到其他机器各用户的 `~/.ssh`目录下。

脚本如下 `↓`

``` bash
#!/bin/bash
 
#@HEADER
# ***************************************************
#
# CIS: Cluster Installation Script
#
# Contact:
# Liu Quan (liuquan2049@buaa.edu.cn)
#
# ***************************************************
#@HEADER

NODE_NUMS=12
ROOT_PASSWD=root
USER_PASSWD=lq


# ===================================================
#ssh_keygen  生成当前用户的密钥 -q quiet
#--- overwrite?  y
ssh_keygen()
{
    expect -c "set timeout -1;
		spawn ssh-keygen -t rsa -P \"\" -f $HOME/.ssh/id_rsa 
		expect {
		    *Overwrite*(y/n)?* {send -- \"y\r\";exp_continue;}
		    eof		{exit 0;}		
		}";
}

# ssh_keygen_all 生成所有用户密钥，包括lq@mic1
# --- $1 --- 用户名
ssh_keygen_all()
{
for i in $(seq 1 12)
do
	if [ $i != 1 -o $1 != "root" ];then
	    ssh $1@mic$i "ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa "
	else 
	echo 23333333333333333333333333333333333333333333333333333333333333
	fi
done
}



# ssh_copy_id 配置当前用户无密码访问hosts中其他节点
# --- $1 --- 当前用户名
# --- $2 --- 当前用户密码
ssh_copy_id()
{
for i in $(seq 1 $NODE_NUMS)
do
node_id=$i
    expect -c "set timeout -1; 
    spawn ssh-copy-id $1@mic$node_id;
    expect {
    	 *(yes/no)* {send -- yes\r;exp_continue;}
         *password:* {send -- $2\r;exp_continue;}
         eof        {exit 0;}
    }";

done
sleep 1
}

# scp_pub 将其他用户公钥复制到mic1
# --- $1 --- 用户名
scp_pub()
{
for i in $(seq 1 $NODE_NUMS)
do
	scp $1@mic$i:~/.ssh/id_rsa.pub tmp
   	cat tmp >> /root/.ssh/authorized_keys
done

rm -f tmp
}

# scp_aukeys 将已认证公钥复制到其他机器
scp_aukeys()
{
for i in $(seq 1 $NODE_NUMS)
do
	scp /root/.ssh/authorized_keys $1@mic$i:~/.ssh/authorized_keys
	scp /root/.ssh/known_hosts     $1@mic$i:~/.ssh/known_hosts
done
}

#=================================================================

# main

rm ~/.ssh/known_hosts

echo -e "\033[32m 开始生成用户密钥  ...  \033[0m"
sleep 1
ssh_keygen
echo -e "\033[32m 用户密钥生成 完成！  \033[0m"
sleep 1
echo 
echo -e "\033[32m 开始配置无密码访问 ...  \033[0m"
sleep 1
ssh_copy_id root $ROOT_PASSWD
ssh_copy_id lq   $USER_PASSWD
echo -e "\033[32m 对其他主机无密码访问配置 完成！ \033[0m"
sleep 1
echo 
echo -e "\033[32m 开始生成其他主机用户密钥  ...  \033[0m"
sleep 1
ssh_keygen_all root
ssh_keygen_all lq
echo -e "\033[32m 其他主机用户密钥生成 完成! \033[0m"
sleep 1
echo 
echo -e "\033[32m 开始加入其他主机公钥  ...  \033[0m"
sleep 1
scp_pub root
scp_pub lq
echo -e "\033[32m 加入其他主机公钥 完成！  \033[0m"
sleep 1
echo 
echo -e "\033[32m 开始分发可用的认证公钥  ...  \033[0m"
sleep 1
scp_aukeys root
scp_aukeys lq
echo -e "\033[32m 可用的认证密钥分发 完成！  ...  \033[0m"
sleep 1
echo 
echo -e "\033[32m 集群无密码访问配置完成  \033[0m"
sleep 1
```

<br>
<br>
<br>

## 四、InfiniBand 配置

如果我们有InfiniBand 交换机的话，InfiniBand的配置应放在第一步来做，因为它关系到了集群的网络互联。  
如果手头上暂时没有InfiniBand交换机的话，也可以在这里先把HCA卡的驱动程序装上，这样之后把IB线插上之后，就可以直接用了。 （当然也需要修改 `hosts` 中的IP，并重启守护进程）

### 1. 安装TCL TK

``` bash
$ yum install tcl tk -y
```
<br>

#### 2. 安装 OFED驱动 

- 在官网中下载驱动程序

    Mellanox Ifiniband驱动:  http://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux_sw_drivers

    Redhat/Centos 7.3 版本: http://www.mellanox.com/page/mlnx_ofed_eula?mtag=linux_sw_drivers&mrequest=downloads&mtype=ofed&mver=MLNX_OFED-4.0-2.0.0.1&mname=MLNX_OFED_LINUX-4.0-2.0.0.1-rhel7.3-x86_64.tgz



- 解压并安装

``` bash
$ wget http://www.mellanox.com/page/mlnx_ofed_eula?mtag=linux_sw_drivers&mrequest=downloads&mtype=ofed&mver=MLNX_OFED-4.0-2.0.0.1&mname=MLNX_OFED_LINUX-4.0-2.0.0.1-rhel7.3-x86_64.tgz 

$ tar xvf MLNX_OFED_LINUX-4.0-2.0.0.1-rhel7.3-x86_64.tgz
$ cd MLNX_OFED_LINUX-4.0-2.0.0.1-rhel7.3-x86_64
$ ./mlnxofedinstall
```

正常安装完成后输出如下，从以下输出可以了解到 `HCA卡` 的基本信息，如类型为EDR (100Gb/s)， 版本为 ConnectX-4。

``` bash
Device #1:
----------

  Device Type:      ConnectX4
  Part Number:      MCX456A-ECA_Ax
  Description:      ConnectX-4 VPI adapter card; EDR IB (100Gb/s) and 100GbE; dual-port QSFP28; PCIe3.0 x16; ROHS R6
  PSID:             MT_2190110032
  PCI Device Name:  85:00.0
  Base GUID:        e41d2d0300a5f102
  Versions:         Current        Available     
     FW             12.18.2000     12.18.2000    
     PXE            3.5.0110       3.5.0110      

  Status:           Up to date


Log File: /tmp/MLNX_OFED_LINUX-4.0-2.0.0.1.4161162.logs/fw_update.log
To load the new driver, run:
/etc/init.d/openibd restart
```


- IP 配置

创建文件 `/etc/sysconfig/network-scripts/ifcfg-ib0` ，写入以下内容并修改 `IPADDR` 和 `NETWORK` 
  
``` bash
DEVICE=ib0
BOOTPROTO=static
ONBOOT=yes
NM_CONTROLLED=yes
NETMASK=255.255.255.0
IPADDR=10.4.9.101
BROADCAST=10.255.255.255
NETWORK=10.4.9.0
```

这里配置完IP后，需要修改各个机器上的 `hosts` 文件。

- 开机启动

``` bash
$ systemctl enable opensmd
$ systemctl enable openibd
```

- 重启网络接口和服务

``` bash
$ ifdown ib0
$ ifup   ib1
$ systemctl restart opensmd
$ systemctl restart openibd
```

- 查看ib 状态

``` bash
$ ibstatus
CA 'mlx5_0'
	CA type: MT4115
	Number of ports: 1
	Firmware version: 12.17.2020
	Hardware version: 0
	Node GUID: 0xe41d2d0300a5f0d6
	System image GUID: 0xe41d2d0300a5f0d6
	Port 1:
		State: Active             #连接正常的状态是 Active
		Physical state: LinkUp    #连接正常的状态是 LinkUp
		Rate: 10
		Base lid: 65535
		LMC: 0
		SM lid: 0
		Capability mask: 0x2651e848
		Port GUID: 0xe41d2d0300a5f0d6
		Link layer: InfiniBand
CA 'mlx5_1'
	CA type: MT4115
	Number of ports: 1
	Firmware version: 12.17.2020
	Hardware version: 0
	Node GUID: 0xe41d2d0300a5f0d7
	System image GUID: 0xe41d2d0300a5f0d6
	Port 1:
		State: Down
		Physical state: Disabled
		Rate: 10
		Base lid: 3
		LMC: 0
		SM lid: 3
		Capability mask: 0x2651e848
		Port GUID: 0xe41d2d0300a5f0d7
		Link layer: InfiniBand

$ ibhosts  #输出信息待补充
```

<br>

### 3. 重启

`opensmd` 服务会和其他服务冲突，目前觉得最好的解决办法是在第一次配置完后设置服务开机启动并直接重启机器，之后一切都正常。

<br>
<br>
<br>

## 五、NFS文件系统 exports

NFS 是Network File System的缩写，即网络文件系统。一种使用于分散式文件系统的协定，由Sun公司开发，于1984年向外公布。功能是通过网络让不同的机器、不同的操作系统能够彼此分享个别的数据，让应用程序在客户端通过网络访问位于服务器磁盘中的数据，是在类Unix系统间实现磁盘文件共享的一种方法。

NFS 的基本原则是“容许不同的客户端及服务端通过一组RPC分享相同的文件系统”，它是独立于操作系统，容许不同硬件及操作系统的系统共同进行文件的分享。

NFS在文件传送或信息传送过程中依赖于RPC协议。RPC，远程过程调用 (Remote Procedure Call) 是能使客户端执行其他系统中程序的一种机制。NFS本身是没有提供信息传输的协议和功能的，但NFS却能让我们通过网络进行资料的分享，这是因为NFS使用了一些其它的传输协议。而这些传输协议用到这个RPC功能的。可以说NFS本身就是使用RPC的一个程序。或者说NFS也是一个RPC SERVER。所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。

<br>

### 1. 安装NFS服务

- nfs-utils :包括基本的NFS命令与监控程序
- rpcbind   :支持安全NFS RPC服务的连接

``` bash
$ yum install rpcbind nfs-utils -y
```

<br>

### 2. NFS系统守护进程

- nfsd：基本的NFS守护进程，主要功能是管理客户端是否能够登录服务器；
- mountd：RPC安装守护进程，主要功能是管理NFS的文件系统。当客户端顺利通过nfsd登录NFS服务器后，在使用NFS服务所提供的文件前，还必须通过文件使用权限的验证。它会读取NFS的配置文件 `/etc/exports` 来对比客户端权限。
- rpcbind：主要功能是进行端口映射工作。当客户端尝试连接并使用RPC服务器提供的服务（如NFS服务）时，rpcbind会将所管理的与服务对应的端口提供给客户端，从而使客户可以通过该端口向服务器请求服务。

设置NFS守护进程开机启动

``` bash
$ systemctl enable rpcbind
$ systemctl enable nfs
```

<br>

### 3. NFS服务器配置

>NFS 常用配置目录  
/etc/exports                            NFS服务的主要配置文件  
/usr/sbin/exportfs                      NFS服务的管理命令  
/usr/sbin/showmount                     客户端的查看命令  
/var/lib/nfs/etab                       记录NFS分享出来的目录的完整权限设定值
/var/lib/nfs/xtab                       记录曾经登录过的客户端信息  

NFS服务的配置文件为 `/etc/exports`，这个文件是NFS的主要配置文件，不过系统并没有默认值，所以这个文件不一定会存在，可能要使用vim手动建立，然后在文件里面写入配置内容。

/etc/exports文件内容格式：

```
<输出目录> [客户端1 选项（访问权限,用户映射,其他）] [客户端2 选项（访问权限,用户映射,其他）]
```

目前我的配置选项为 `↓`

``` 
/home/cluster *(rw,no_all_squash,no_root_squash,fsid=0,insecure)
```
- 输出目录：

    输出目录是指NFS系统中需要共享给客户机使用的目录，客户端和服务端目录必须一致，否则会出现程序无法在多机运行的情况。

- 客户端：

    客户端是指网络中可以访问这个NFS输出目录的计算机， 客户端常用的指定方式为：

    - 指定ip地址的主机：10.4.9.101  
    - 指定子网中的所有主机：10.4.9.0/24 10.4.9.0/255.255.255.0  
    - 指定域名的主机：nfs.cnhzz.com  
    - 指定域中的所有主机：*.cnhzz.com  
    - 所有主机：*  

- 选项：

    选项用来设置输出目录的访问权限、用户映射等。我们把访问权限设为 `rw`，为使客户端的NFS目录权限正常，应选择 `no_all_squash` 和 `no_root_squash` 选项。

    NFS主要有3类选项：

    - 访问权限选项  

        - 设置输出目录只读：ro  
        - 设置输出目录读写：rw  

    - 用户映射选项 (关于用户访问权限，后文将详述)

        - all_squash：将远程访问的所有普通用户及所属组都映射为匿名用户或用户组（nfsnobody）
        - no_all_squash：与 `all_squash` 取反（默认设置）
        - root_squash：将root用户及所属组都映射为匿名用户或用户组（默认设置）
        - no_root_squash：与 `rootsquash` 取反
        - anonuid=xxx：将远程访问的所有用户都映射为匿名用户，并指定该用户为本地用户（UID=xxx）
        - anongid=xxx：将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组账户为本地用户组账户（GID=xxx）

    - 其它选项    

        - secure：限制客户端只能从小于1024的tcp/ip端口连接nfs服务器（默认设置）
        - insecure：允许客户端从大于1024的tcp/ip端口连接服务器
        - sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性
        - async：将数据先保存在内存缓冲区中，必要时才写入磁盘
        - wdelay：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率（默认设置）
        - no_wdelay：若有写操作则立即执行，应与sync配合使用
        - subtree：若输出目录是一个子目录，则nfs服务器将检查其父目录的权限(默认设置)
        - no_subtree：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率

<br>

#### 4. 服务端及客户端启动NFS


配置好 `/etc/exports` 文件后，就可以启动NFS服务器了。

- 服务端

创建目录并修改文件夹权限
``` bash
$ mkdir /home/cluster
$ chmod 777 /home/cluster #在服务器端打开NFS目录 rwx 权限
```
`/home/cluster` 这里也可以选择其他目录，为了方便管理，推荐在 `/home` 目录下创建NFS目录。

重启NFS服务
``` bash 
$ systemctl restart rpcbind
$ systemctl restart nfs
```
- 客户端

将服务端NFS目录挂载到本机相同目录下
``` bash
$ mount [-t nfs] <hostname>:/home/cluster /home/cluster
```
<br>

#### 5. NFS 访问权限测试

- 测试文件的所有者是否正常
``` bash
[root@mic11 cluster]# touch root
[root@mic11 cluster]# su lq
[lq@mic11 cluster]$ touch lq
[lq@mic11 cluster]$ ll
总用量 0
-rw-rw-r-- 1 lq   lq   0 5月  17 23:36 lq
-rw-r--r-- 1 root root 0 5月  17 23:36 root
```

>- 关于权限的分析
>1. 客户端连接时候，对普通用户的检查
>    - 如果明确设定了普通用户被压缩的身份，那么此时客户端用户的身份转换为指定用户；
>    - 如果 NFS 服务端 上面有同名用户，且 `UID` 和 `GID` 一致，那么此时客户端登录账户的身份转换为NFS 服务端 上面的同名用户；
>    - 如果没有明确指定，也没有同名用户，那么此时 用户身份被压缩成nfsnobody；
>2. 客户端连接的时候，对root的检查
>    - 如果设置no_root_squash，那么此时root用户的身份被压缩为NFS server上面的>root；
>    - 如果设置了all_squash、anonuid、anongid，此时root 身份被压缩为指定用户；
>    - 如果没有明确指定，此时root用户被压缩为nfsnobody；
>    - 如果同时指定no_root_squash与all_squash 用户将被压缩为 nfsnobody，如果设置>了anonuid、anongid将被压缩到所指定的用户与组；

<br>

#### 6. 相关命令

``` bash
$ nfsstat
```

查看NFS的运行状态，对于调整NFS的运行有很大帮助。

``` bash 
$ rpcinfo
```

查看rpc执行信息，可以用于检测rpc运行情况的工具，利用rpcinfo -p 可以查看出RPC开启的端口所提供的程序有哪些。

``` bash
$ showmount
```

-a 显示已经于客户端连接上的目录信息  
-e `IP` 或者 `hostname` 显示此IP地址分享出来的目录

``` bash 
$ netstat
```

可以查看出nfs服务开启的端口，其中 nfs 开启的是2049，rpcbind 开启的是 111，其余则是rpc开启的。

<br>

#### 7. 特别注意！！！ 先卸载，再关机！！

卸载NFS目录

``` bash
$ umount /home/cluster
```


NFS不太稳定，请注意规范操作。  
若仍未卸载NFS客户端，服务端直接关机会使NFS目录的文件句柄丢失的问题。NFS目录的读写量过大也可能出现崩溃的情况。（ASC16曾经出现过）

NFS 客户端关机的时候一点要确保NFS服务关闭，没有客户端处于连接状态！通过 `showmount -a` 或 `fuser /home/cluster`  可以查看，如果有的话通过 `kill`, `killall`, `pkill`, `fuser -k /home/cluster` 来结束。

#### 8. 通过挂载 tmpfs 提高NFS文件系统IO速度

在IO密集的程序中，磁盘的读写速度会限制程序的运行速度和优化效果。我们可以直接把一部分内存用作NFS目录，以提升IO速度，这种方式的速度要远快于普通机械硬盘，和SSD相比在读写速度上也更有优势。

- 挂载 tmpfs

``` bash
$ mount -t tmpfs -o size=4G /home/cluster
$ df -h
Filesystem           Size  Used Avail Use% Mounted on
/dev/mapper/cl-root   50G   19G   32G  37% /
devtmpfs              12G     0   12G   0% /dev
tmpfs                 12G  2.4M   12G   1% /dev/shm
tmpfs                 12G  322M   12G   3% /run
tmpfs                 12G     0   12G   0% /sys/fs/cgroup
/dev/sda1           1014M  174M  841M  18% /boot
/dev/mapper/cl-home   74G   54M   74G   1% /home
tmpfs                2.4G     0  2.4G   0% /run/user/0
tmpfs                2.4G   16K  2.4G   1% /run/user/42
tmpfs                2.4G     0  2.4G   0% /run/user/1000
tmpfs                4.0G     0  4.0G   0% /home/cluster  #成功挂载
``` 
也可以直接通过 `mount` 命令来查看是否成功挂载

``` bash
$ mount | grep 'cluster'
tmpfs on /home/cluster type tmpfs (rw,relatime,seclabel,size=4194304k)
```

注意正确的顺序是：  
    1. 先把 `tmpfs` 挂载到 服务端的NFS目录上   
    2. 再把NFS目录共享给客户端，  
    3. 客户端挂载NFS目录。  
    
这里先说明了NFS的配置和挂载，相信对NFS熟悉的话，`tmpfs` 也就不在话在了。

- 卸载 tmpfs

``` bash
$ umount /home/cluster
```

注意若使用tmpfs，在卸载后，目录中的数据会丢失，故平时不推荐使用。在需要用到时，务必在卸载之前备份好重要数据。  

---
 
  